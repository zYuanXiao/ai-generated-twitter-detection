# AI-Generated Twitter Detection  
*A CLIP-Based Approach to Identifying AI-Generated Tweets*  
**Yuqing Fan & Zhiyuan Xiao â€” CSC 245 (Fall 2025)**

## 1. Motivation

AI-generated textâ€“image content is increasingly common on social media.  
Real and synthetic tweets often appear similar, and manual inspection is slow and inconsistent.  
This project asks:

**Can we detect whether an imageâ€“text pair is real by measuring their semantic alignment?**

We develop a scalable multimodal detector using CLIP.


## 2. Method Overview

### ğŸ”¹ Dataset Construction  
We create real and synthetic tweet pairs.  
AI-generated captions follow a controlled prompt:

> â€œLook at the image internally. Do not describe the image. Do not explain the image. Write a tweet-style post inspired by its mood or message, keeping it extremely short: 1-2 sentences, under 30 words ...  You may include one or two natural hashtags ... Avoid descriptions, summaries, or analytical language. Output only the tweet.â€

This generates natural AI-style captions.

### ğŸ”¹ Model Architecture  
We use **CLIP ViT-L/14-336** to extract:

- image embeddings  
- text embeddings  
- cosine similarity  

Feature vector:
[image_emb âˆ¥ text_emb âˆ¥ cosine_sim]

Classifier:  
A 2-layer MLP (hidden size = 512).

Benefits:
- Embeddings retain rich multimodal signals  
- Goes beyond a single similarity score  
- Captures tweet-specific non-linear patterns  


## 3. Results

### ğŸ¯ Accuracy: **87.37%**  
### ğŸ¯ F1 Score: **0.88**

<img src="Accuracy_and_F1_score.png" width="650"/>

Model behavior:
- Detects most real/fake tweets accurately  
- Balanced precision, recall, and f1  
- Moderate errors:
  - **False Positive**: real image paired with very short generated text  
  - **False Negative**: artistic images or very long text (>77 tokens)

## 4. Interpretability

We analyze:

- **Modal-level contributions** (image vs text vs similarity)  
- **Token contributions** (mask each token)  
- **Image occlusion heatmaps** (patch sensitivity)

Example:

<img src="contrib_0cd5khj3wo5a1.png" width="500"/>

Insights:
- Highlights sensitive image regions  
- Identifies influential words  
- Explains misclassifications  
- Evaluates robustness to text length and visually complex artwork


## 5. Method Improvement & Benchmark Plan

### âœ” Performance Comparison
- **Ours (CLIP + MLP)**: 87.37% accuracy  
- **Zero-shot CLIP**: 55.01% accuracy  
- Large improvement in both accuracy and F1.

### âœ” Future Benchmark (Similar to GenEval)
Goal: Evaluate *how real* an AI-generated tweet appears.

Possible benchmark dimensions:
- Semantic consistency  
- Visual artifact detection  
- Text style naturalness  
- Cross-model robustness (Midjourney, SD3, DALLÂ·E, etc.)  
- Reasoning-based mismatch detection  

## 6. File Structure
```bash
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ best_clip_match_classifier.pt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ archive/
â”‚   â”œâ”€â”€ fakeV2/
â”‚   â”œâ”€â”€ real_fake/
â”‚   â”œâ”€â”€ qwen3_prompt.py
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ val.csv
â”‚   â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ build_csv.py
â”œâ”€â”€ build_csv_adapted.py
â”œâ”€â”€ clip_tweet_match.py
â”œâ”€â”€ plot_eval.py
â”œâ”€â”€ visualize_part_contribution.py
â”œâ”€â”€ Accuracy_and_F1_score.png
â”œâ”€â”€ contrib_0cd5khj3wo5a1.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```

## 7. Usage

### Install dependencies

```bash
pip install -r requirements.txt
```
### Train model
```bash
python clip_tweet_match.py
```
### Evaluate
```bash
python plot_eval.py
```
### Run interpretability
```bash
python visualize_part_contribution.py `
  --ckpt "checkpoints/best_clip_match_classifier.pt" `
  --image "data/real_fake/real/0cd5khj3wo5a1.jpg" `
  --text "Chasing the glow in the dark, where colors bleed into dreams. ğŸŒŒ #VividSoul" `
  --grid 7 `
  --out "contrib_0cd5khj3wo5a1.png"
```
